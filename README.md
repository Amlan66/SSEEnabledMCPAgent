# ğŸ§  Cortex-R: SSE-Enabled MCP Agent

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![MCP Protocol](https://img.shields.io/badge/MCP-1.6.0-green.svg)](https://github.com/modelcontextprotocol/python-sdk)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

> **An advanced AI agent with multi-modal perception, persistent memory, and tool-using capabilities, powered by the Model Context Protocol (MCP) with both STDIO and SSE transports.**

Cortex-R is a reasoning-driven AI agent that can solve complex tasks step-by-step using external tools, memory retrieval, and strategic planning. It features Telegram integration for seamless conversational AI interactions in group chats, making it accessible from anywhere.

---

## ğŸŒŸ Key Features

### ğŸ¯ Core Capabilities
- **Multi-Step Reasoning**: Breaks down complex problems into manageable steps
- **Tool-Using Agent**: Dynamically selects and executes tools based on task requirements
- **Persistent Memory**: Uses FAISS-based vector storage for contextual memory retrieval
- **Multi-Modal Processing**: Handles text, PDFs, images, and web content
- **Strategic Planning**: Conservative, retry-once, and explore-all strategies
- **Dual Interface**: CLI mode for development, Telegram mode for production use

### ğŸ”§ Tool Ecosystem
- **ğŸ§® Mathematical Tools** (30+ operations): Arithmetic, trigonometry, factorials, exponentials, custom Python sandbox
- **ğŸ“„ Document Processing**: PDF extraction, web scraping, semantic chunking, image captioning
- **ğŸ” Web Search**: DuckDuckGo integration with intelligent rate limiting
- **ğŸ“± Telegram Integration**: Real-time message handling via SSE protocol

### ğŸ—ï¸ Architecture Highlights
- **MCP Protocol**: Unified tool abstraction with support for STDIO and SSE transports
- **Modular Design**: Perception â†’ Memory â†’ Decision â†’ Action loop
- **Async-First**: Built on asyncio for concurrent operations
- **Profile-Based Configuration**: YAML-based agent personas and strategies

---

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Architecture](#-architecture)
- [Usage Modes](#-usage-modes)
- [MCP Servers](#-mcp-servers)
- [Telegram Integration](#-telegram-integration)
- [Configuration](#-configuration)
- [Agent Loop](#-agent-loop)
- [Memory System](#-memory-system)
- [Tool Development](#-tool-development)
- [Examples](#-examples)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## âš¡ Quick Start

### Prerequisites
- Python 3.11 or higher
- `uv` package manager (recommended) or `pip`
- Telegram account (for Telegram mode)
- Gemini API key (for LLM operations)

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd SSEEnabledMCPAgent

# Install dependencies using uv (recommended)
uv sync

# Or using pip
pip install -r requirements.txt

# Create environment file
cp .env.example .env
# Edit .env and add your API keys
```

### Basic Usage - CLI Mode

```bash
# Start the agent in CLI mode
uv run python agent.py --mode cli

# Or simply
uv run python agent.py
```

Example query:
```
What is the square root of 144?
```

### Telegram Mode Setup

See [ğŸ“± Telegram Integration](#-telegram-integration) for detailed setup instructions.

**Quick version:**
```bash
# Terminal 1: Start Telegram SSE Server
uv run python mcp_telegram_server.py

# Terminal 2: Start Agent in Telegram Mode
uv run python agent.py --mode telegram
```

For Windows users, use the provided batch scripts:
```bash
# Terminal 1
start_telegram_server.bat

# Terminal 2
start_telegram_agent.bat
```

---

## ğŸ”§ Installation

### System Requirements
- **OS**: Windows, macOS, or Linux
- **Python**: 3.11+
- **RAM**: 4GB minimum (8GB recommended)
- **Storage**: 1GB for dependencies and models

### Detailed Installation Steps

1. **Install Python 3.11+**
   ```bash
   # Verify Python version
   python --version  # Should be 3.11 or higher
   ```

2. **Install uv Package Manager** (recommended)
   ```bash
   # macOS/Linux
   curl -LsSf https://astral.sh/uv/install.sh | sh

   # Windows (PowerShell)
   powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
   ```

3. **Clone and Setup**
   ```bash
   git clone <repository-url>
   cd SSEEnabledMCPAgent
   
   # Install all dependencies
   uv sync
   ```

4. **Configure Environment**
   ```bash
   # Create .env file
   touch .env  # or New-Item .env on Windows
   ```

   Add the following to `.env`:
   ```env
   # Required for LLM operations
   GEMINI_API_KEY=your_gemini_api_key_here
   
   # Required for Telegram mode
   TELEGRAM_BOT_TOKEN=your_telegram_bot_token_here
   
   # Optional: Ollama configuration (if using local models)
   OLLAMA_URL=http://localhost:11434
   ```

5. **Verify Installation**
   ```bash
   # Test CLI mode
   uv run python agent.py --mode cli
   
   # You should see:
   # ============================================================
   # ğŸ§  Cortex-R Agent Ready
   # ğŸ“ Mode: CLI
   # ============================================================
   ```

---

## ğŸ—ï¸ Architecture

Cortex-R implements a sophisticated agent architecture based on the **Perception-Memory-Decision-Action** cycle:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER INPUT / QUERY                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ§  PERCEPTION MODULE                      â”‚
â”‚  - Extracts intent, entities, and tool hints from query     â”‚
â”‚  - Powered by LLM (Gemini)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ’¾ MEMORY RETRIEVAL                       â”‚
â”‚  - FAISS-based vector search                                â”‚
â”‚  - Retrieves relevant past tool outputs and facts           â”‚
â”‚  - Contextual memory filtering by session/type              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ¯ DECISION / PLANNING                    â”‚
â”‚  - Strategic planning based on agent profile                â”‚
â”‚  - Tool selection and parameter generation                  â”‚
â”‚  - Conservative / Retry-once / Explore-all strategies       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    âš™ï¸ ACTION EXECUTION                       â”‚
â”‚  - MCP Tool Dispatcher (MultiMCP)                           â”‚
â”‚  - Dynamic routing to STDIO or SSE servers                  â”‚
â”‚  - Result parsing and memory storage                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                  [Loop or FINAL_ANSWER]
```

### Core Components

#### 1. **Agent Loop** (`core/loop.py`)
The main orchestrator that runs the perception-memory-decision-action cycle for up to N steps (default: 6).

#### 2. **Context Manager** (`core/context.py`)
Maintains session state, including:
- Current step number
- Memory trace
- Tool call history
- Agent profile and configuration

#### 3. **MCP Session Handler** (`core/session.py`)
Manages connections to multiple MCP servers:
- **STDIO Transport**: For local tool servers (math, documents, websearch)
- **SSE Transport**: For remote servers (Telegram integration)

#### 4. **Perception Module** (`modules/perception.py`)
Extracts structured information from user queries:
- **Intent**: High-level goal
- **Entities**: Keywords and values
- **Tool Hint**: Suggested MCP tool

#### 5. **Memory System** (`modules/memory.py`)
FAISS-based vector database for:
- Tool output storage
- Semantic search
- Context retrieval

#### 6. **Decision Engine** (`modules/decision.py`)
Generates structured plans:
- Tool selection
- Parameter extraction
- FINAL_ANSWER detection

---

## ğŸ® Usage Modes

Cortex-R supports two operational modes:

### 1. CLI Mode (Interactive Command Line)

Best for: Development, testing, and single queries

```bash
# Start CLI mode
uv run python agent.py --mode cli

# Or use the shortcut
uv run python agent.py
```

**Example Session:**
```
============================================================
ğŸ§  Cortex-R Agent Ready
ğŸ“ Mode: CLI
============================================================

ğŸ§‘ What do you want to solve today? â†’ Find the ASCII values of INDIA and sum their exponentials

[agent] Starting session: session-1234567890-abc123
[loop] Step 1 of 6
[perception] Intent: Convert string to ASCII and compute exponential sum
[memory] Retrieved 0 memories
[plan] FUNCTION_CALL: strings_to_chars_to_int|input={"string": "INDIA"}
[action] strings_to_chars_to_int â†’ [73, 78, 68, 73, 65]
[loop] Step 2 of 6
[plan] FUNCTION_CALL: int_list_to_exponential_sum|input={"int_list": [73, 78, 68, 73, 65]}
[action] int_list_to_exponential_sum â†’ 1.234567e+35

============================================================
ğŸ’¡ Final Answer:
============================================================
The ASCII values of "INDIA" are [73, 78, 68, 73, 65], and the sum 
of their exponentials is approximately 1.23 Ã— 10^35.
============================================================
```

### 2. Telegram Mode (Production Bot)

Best for: Production deployment, multi-user access, remote operation

**Setup Process:**

1. **Create Telegram Bot**
   - Open Telegram and search for `@BotFather`
   - Send `/newbot` and follow instructions
   - Copy the bot token
   - Send `/mybots` â†’ Select bot â†’ Bot Settings â†’ Group Privacy â†’ **Turn OFF**

2. **Configure Environment**
   ```bash
   # Add to .env
   TELEGRAM_BOT_TOKEN=1234567890:ABCdefGHIjklMNOpqrsTUVwxyz
   ```

3. **Add Bot to Group**
   - Create a Telegram group
   - Add your bot as a member

4. **Start System**
   ```bash
   # Terminal 1: Telegram SSE Server
   uv run python mcp_telegram_server.py
   
   # Terminal 2: Agent
   uv run python agent.py --mode telegram
   ```

5. **Test**
   Send a message in your Telegram group:
   ```
   What is 25 * 4?
   ```

**How It Works:**
```
[Telegram Group Message]
          â†“
[Telegram Bot (python-telegram-bot)]
          â†“
[Message Queue (asyncio.Queue)]
          â†“
[MCP SSE Server (FastAPI + SSE)]
          â†“ HTTP SSE Protocol
[Agent Loop (Perception â†’ Memory â†’ Decision â†’ Action)]
          â†“
[Tool Execution (math, docs, web)]
          â†“
[send_telegram_reply()]
          â†“
[Telegram Group]
```

**Telegram-Specific Features:**
- âœ… Automatic message acknowledgment ("Processing...")
- âœ… Markdown formatting support
- âœ… Long message splitting (4000 char chunks)
- âœ… Continuous operation (handles multiple queries)
- âœ… Error handling with user-friendly messages

---

## ğŸ”§ MCP Servers

Cortex-R uses the Model Context Protocol (MCP) to abstract tool functionality. Each server exposes a set of tools via MCP.

### Server 1: Mathematical Operations (`mcp_server_1.py`)

**Tools Provided:**
- Basic arithmetic: `add`, `subtract`, `multiply`, `divide`
- Advanced math: `sqrt`, `cbrt`, `power`, `factorial`
- Trigonometry: `sin`, `cos`, `tan`
- Utilities: `remainder`, `strings_to_chars_to_int`, `int_list_to_exponential_sum`
- Code execution: `run_python_sandbox` (safe Python evaluation)
- System: `run_shell_command`, `run_sql_query`

**Example Tool Usage:**
```python
# Tool: add
add(a=10, b=5) â†’ 15

# Tool: strings_to_chars_to_int
strings_to_chars_to_int(string="INDIA") â†’ [73, 78, 68, 73, 65]

# Tool: run_python_sandbox
run_python_sandbox(code="import math; result = math.sqrt(49)") â†’ "7.0"
```

**Transport:** STDIO

### Server 2: Document Processing (`mcp_server_2.py`)

**Tools Provided:**
- `search_documents`: Semantic search over indexed documents (FAISS)
- `extract_webpage`: Convert web pages to markdown (Trafilatura)
- `extract_pdf`: Convert PDFs to markdown with image captioning (PyMuPDF4LLM)

**Document Indexing:**
- Automatically processes files in `documents/` folder
- Supports: PDF, DOCX, HTML, TXT, Markdown
- Creates FAISS index with semantic chunks
- Image captioning via Ollama (gemma3:12b)
- Incremental updates (only processes changed files)

**Semantic Chunking:**
Uses LLM-based topic detection to split documents intelligently:
```python
# Example: Processing a multi-topic document
Document â†’ LLM Topic Splitter â†’ Semantic Chunks â†’ FAISS Index
```

**Example Queries:**
```
"What do you know about DLF apartments?"
â†’ Searches indexed documents for relevant content
â†’ Returns top-k matches with source attribution
```

**Transport:** STDIO

### Server 3: Web Search (`mcp_server_3.py`)

**Tools Provided:**
- `search`: DuckDuckGo search with results formatting
- `fetch_content`: Extract and parse webpage content

**Features:**
- Rate limiting (30 requests/minute)
- User-agent rotation
- Result deduplication
- Content sanitization
- Timeout handling

**Example Usage:**
```python
# Tool: search
search(query="F1 2024 standings", max_results=5)
â†’ Returns formatted search results with titles, URLs, and snippets

# Tool: fetch_content
fetch_content(url="https://example.com")
â†’ Returns cleaned webpage text (max 8000 chars)
```

**Transport:** STDIO

### Server 4: Telegram Integration (`mcp_telegram_server.py`)

**Tools Provided:**
- `get_telegram_query`: Wait for and retrieve Telegram messages
- `send_telegram_reply`: Send responses back to Telegram
- `check_telegram_status`: Monitor bot health

**Architecture:**
- FastMCP SSE server on `http://localhost:8000`
- `python-telegram-bot` for Telegram API
- asyncio.Queue for message buffering
- Markdown message formatting

**Transport:** SSE (Server-Sent Events)

---

## ğŸ“± Telegram Integration

### Why Telegram?
- **Accessible Anywhere**: No need to be at your computer
- **Multi-User**: Share one agent across team members
- **Group Conversations**: Collaborative AI interactions
- **Rich Formatting**: Markdown support for better readability
- **Push Notifications**: Instant responses

### Setup Guide

See [TELEGRAM_SETUP_GUIDE.md](TELEGRAM_SETUP_GUIDE.md) for comprehensive instructions.

**Quick Setup:**

1. **Create Bot** (via @BotFather)
   ```
   /newbot
   Bot name: My AI Agent
   Username: my_ai_agent_bot
   â†’ Copy token
   ```

2. **Configure Privacy**
   ```
   /mybots â†’ Select bot â†’ Bot Settings â†’ Group Privacy â†’ Turn OFF
   ```

3. **Environment Setup**
   ```env
   TELEGRAM_BOT_TOKEN=your_token_here
   ```

4. **Run System**
   ```bash
   # Terminal 1
   uv run python mcp_telegram_server.py
   
   # Terminal 2
   uv run python agent.py --mode telegram
   ```

### Telegram Commands

Currently, the bot responds to all text messages. Future versions will support commands:

**Planned Commands:**
- `/help` - Show available commands
- `/status` - Check agent status
- `/clear` - Clear conversation memory
- `/tools` - List available tools
- `/mode <strategy>` - Change planning strategy

---

## âš™ï¸ Configuration

### Agent Profiles (`config/profiles.yaml`)

Define agent behavior, strategies, and tool configurations:

```yaml
agent:
  name: Cortex-R
  id: cortex_r_001
  description: A reasoning-driven AI agent

strategy:
  type: conservative  # Options: conservative, retry_once, explore_all
  max_steps: 6

memory:
  top_k: 3
  type_filter: tool_output  # Options: tool_output, fact, query, all
  embedding_model: nomic-embed-text
  embedding_url: http://localhost:11434/api/embeddings

llm:
  text_generation: gemini
  embedding: nomic

persona:
  tone: concise
  verbosity: low
  behavior_tags: [rational, focused, tool-using]

mcp_servers:
  - id: math
    script: mcp_server_1.py
    transport: stdio
    
  - id: documents
    script: mcp_server_2.py
    transport: stdio
    
  - id: websearch
    script: mcp_server_3.py
    transport: stdio
    
  - id: telegram
    transport: sse
    url: http://localhost:8000/sse
```

### Model Configuration (`config/models.json`)

Configure LLM providers and endpoints:

```json
{
  "gemini": {
    "provider": "google",
    "model": "gemini-2.0-flash-exp",
    "api_key_env": "GEMINI_API_KEY",
    "temperature": 0.7
  },
  "ollama": {
    "provider": "ollama",
    "model": "phi4:latest",
    "base_url": "http://localhost:11434"
  }
}
```

---

## ğŸ” Agent Loop

The agent loop is the core execution cycle:

### Loop Structure

```python
for step in range(max_steps):
    # 1. PERCEPTION
    perception = await extract_perception(query)
    
    # 2. MEMORY RETRIEVAL
    memories = memory.retrieve(query, top_k=3)
    
    # 3. DECISION / PLANNING
    plan = await decide_next_action(context, perception, memories, tools)
    
    # 4. ACTION EXECUTION
    if "FUNCTION_CALL" in plan:
        result = await mcp.call_tool(tool_name, arguments)
        memory.add(tool_output)
        query = build_followup_query(result)
    elif "FINAL_ANSWER" in plan:
        break
```

### Exit Conditions

1. **FINAL_ANSWER detected**: Agent provides conclusive answer
2. **Max steps reached**: Prevents infinite loops (default: 6 steps)
3. **Error encountered**: Graceful failure with error message

### Example Multi-Step Execution

**Query:** "Find ASCII values of INDIA and calculate sum of exponentials"

```
Step 1:
  Perception: Convert string to ASCII
  Plan: FUNCTION_CALL: strings_to_chars_to_int(string="INDIA")
  Result: [73, 78, 68, 73, 65]

Step 2:
  Perception: Calculate exponential sum
  Plan: FUNCTION_CALL: int_list_to_exponential_sum(int_list=[73, 78, 68, 73, 65])
  Result: 1.234e+35

Step 3:
  Perception: Task complete
  Plan: FINAL_ANSWER: The sum of exponentials is 1.234e+35
```

---

## ğŸ’¾ Memory System

Cortex-R implements a sophisticated memory system using FAISS for vector storage.

### Memory Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Memory Item                â”‚
â”‚  - text: "tool(args) â†’ result"     â”‚
â”‚  - type: tool_output/fact/query    â”‚
â”‚  - session_id: session-xxx         â”‚
â”‚  - timestamp: 2025-01-01           â”‚
â”‚  - embedding: [768-dim vector]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        FAISS Index (L2)             â”‚
â”‚  - 768-dimensional vectors          â”‚
â”‚  - Fast similarity search           â”‚
â”‚  - Persistent storage               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Types

1. **tool_output**: Results from tool executions
2. **fact**: Extracted knowledge from documents
3. **query**: User queries for context
4. **all**: Retrieve all types

### Memory Retrieval

```python
# Retrieve relevant memories
memories = memory.retrieve(
    query="What is the population of India?",
    top_k=3,
    type_filter="tool_output",
    session_filter="session-123"
)
```

### Memory Storage Location

- **Document Index**: `faiss_index/`
  - `index.bin`: FAISS index file
  - `metadata.json`: Document chunks and metadata
  - `doc_index_cache.json`: File hashes for incremental updates

---

## ğŸ› ï¸ Tool Development

Want to add custom tools? Here's how:

### Creating a New MCP Server

1. **Create Server File** (`mcp_server_custom.py`)

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("custom-tools")

@mcp.tool()
def my_custom_tool(input: str) -> str:
    """
    Description of what the tool does.
    Usage: my_custom_tool|input="example"
    """
    # Your tool logic here
    result = process(input)
    return result

if __name__ == "__main__":
    mcp.run(transport="stdio")
```

2. **Add to profiles.yaml**

```yaml
mcp_servers:
  - id: custom
    script: mcp_server_custom.py
    cwd: /path/to/project
    transport: stdio
```

3. **Test Tool**

```bash
# Restart agent
uv run python agent.py --mode cli

# Query: "Use my custom tool with input 'test'"
```

### Tool Best Practices

- **Clear Descriptions**: Include usage examples in docstrings
- **Type Hints**: Use Pydantic models for complex inputs
- **Error Handling**: Return error messages instead of raising exceptions
- **Logging**: Use `print()` to stderr for debugging (doesn't interfere with MCP protocol)

---

## ğŸ“š Examples

### Example 1: Mathematical Calculation

**Query:** "What is the factorial of 5 plus the square root of 144?"

**Execution:**
```
Step 1: factorial(5) â†’ 120
Step 2: sqrt(144) â†’ 12
Step 3: add(120, 12) â†’ 132
FINAL_ANSWER: 132
```

### Example 2: Document Search

**Query:** "What do you know about Tesla's open innovation?"

**Execution:**
```
Step 1: search_documents("Tesla open innovation")
  â†’ Returns: "Tesla Motors released its patents in 2014 to promote 
     electric vehicle adoption..." [Source: Tesla_Motors_IP_*.pdf]

FINAL_ANSWER: Tesla released its patents in 2014 as part of its open 
innovation strategy to accelerate EV adoption.
```

### Example 3: Web Search + Content Fetch

**Query:** "Who won the 2024 F1 championship?"

**Execution:**
```
Step 1: search("F1 2024 championship winner", max_results=5)
  â†’ Returns: [Result 1: "Max Verstappen wins...", URL: ...]

Step 2: fetch_content(url="...")
  â†’ Returns: "Max Verstappen secured his fourth consecutive F1 world 
     championship..."

FINAL_ANSWER: Max Verstappen won the 2024 F1 world championship.
```

### Example 4: Complex Multi-Tool Query

**Query:** "Find the log value of the amount Anmol Singh paid for his DLF apartment"

**Execution:**
```
Step 1: search_documents("Anmol Singh DLF apartment payment")
  â†’ Returns: "Anmol Singh paid â‚¹2,50,00,000 for his DLF apartment..."

Step 2: run_python_sandbox(code="import math; result = math.log(25000000)")
  â†’ Returns: "17.03"

FINAL_ANSWER: Anmol Singh paid â‚¹2.5 crore for his DLF apartment. 
The natural log of this amount is approximately 17.03.
```

---

## ğŸ› Troubleshooting

### Common Issues

#### 1. "TELEGRAM_BOT_TOKEN not set" Error

**Solution:**
```bash
# Create .env file
echo "TELEGRAM_BOT_TOKEN=your_token_here" > .env
```

#### 2. Bot Doesn't See Messages in Telegram Group

**Solution:**
- Go to @BotFather
- Send `/mybots` â†’ Select bot â†’ Bot Settings â†’ Group Privacy â†’ **Turn OFF**
- Re-add bot to group

#### 3. "Connection refused" Error (Telegram Mode)

**Solution:**
```bash
# Ensure Telegram SSE server is running first
# Terminal 1
uv run python mcp_telegram_server.py

# Wait for "MCP SSE server on http://localhost:8000" message
# Then start agent in Terminal 2
```

#### 4. FAISS Index Not Found

**Solution:**
```bash
# Run document server once to generate index
uv run python mcp_server_2.py

# Wait for indexing to complete
# Then start agent normally
```

#### 5. "Tool 'X' not found on any server"

**Solution:**
- Verify tool name matches exactly (case-sensitive)
- Check `config/profiles.yaml` includes the server
- Restart agent to reload tool list

#### 6. Ollama Connection Error

**Solution:**
```bash
# Ensure Ollama is running
ollama serve

# Pull required models
ollama pull nomic-embed-text
ollama pull gemma3:12b
ollama pull phi4:latest
```

### Debug Mode

Enable verbose logging:

```python
# In agent.py, add at top:
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## ğŸ“‚ Project Structure

```
SSEEnabledMCPAgent/
â”œâ”€â”€ agent.py                      # Main agent entry point
â”œâ”€â”€ mcp_server_1.py               # Math tools server (STDIO)
â”œâ”€â”€ mcp_server_2.py               # Document tools server (STDIO)
â”œâ”€â”€ mcp_server_3.py               # Web search server (STDIO)
â”œâ”€â”€ mcp_telegram_server.py        # Telegram integration (SSE)
â”œâ”€â”€ models.py                     # Pydantic models for tool I/O
â”œâ”€â”€ session.py                    # Session management (deprecated)
â”œâ”€â”€ pyproject.toml                # Project dependencies
â”œâ”€â”€ uv.lock                       # Dependency lock file
â”œâ”€â”€ .env                          # Environment variables (create this)
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ profiles.yaml             # Agent configuration
â”‚   â””â”€â”€ models.json               # LLM provider settings
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ context.py                # Agent context and state
â”‚   â”œâ”€â”€ loop.py                   # Main agent loop
â”‚   â”œâ”€â”€ session.py                # MCP session management
â”‚   â””â”€â”€ strategy.py               # Planning strategies
â”‚
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ perception.py             # Intent and entity extraction
â”‚   â”œâ”€â”€ memory.py                 # FAISS-based memory system
â”‚   â”œâ”€â”€ decision.py               # Planning and tool selection
â”‚   â”œâ”€â”€ action.py                 # Tool execution
â”‚   â”œâ”€â”€ tools.py                  # Tool utilities
â”‚   â””â”€â”€ model_manager.py          # LLM abstraction layer
â”‚
â”œâ”€â”€ documents/                    # Documents for indexing
â”‚   â”œâ”€â”€ *.pdf                     # PDF files
â”‚   â”œâ”€â”€ *.docx                    # Word documents
â”‚   â”œâ”€â”€ *.txt                     # Text files
â”‚   â””â”€â”€ images/                   # Extracted images
â”‚
â”œâ”€â”€ faiss_index/                  # FAISS index storage
â”‚   â”œâ”€â”€ index.bin                 # Vector index
â”‚   â”œâ”€â”€ metadata.json             # Chunk metadata
â”‚   â””â”€â”€ doc_index_cache.json      # File hashes
â”‚
â”œâ”€â”€ start_telegram_server.bat     # Windows: Start Telegram server
â”œâ”€â”€ start_telegram_agent.bat      # Windows: Start agent (Telegram)
â”œâ”€â”€ start_cli_agent.bat           # Windows: Start agent (CLI)
â”‚
â”œâ”€â”€ QUICK_START.md                # Quick reference guide
â”œâ”€â”€ TELEGRAM_SETUP_GUIDE.md       # Detailed Telegram setup
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md     # Technical implementation details
â””â”€â”€ README.md                     # This file
```

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

### Areas for Contribution

1. **New MCP Tools**
   - Add support for more APIs (GitHub, Notion, etc.)
   - Enhance existing tools with more features

2. **Enhanced Memory System**
   - Implement vector compression
   - Add memory pruning strategies
   - Support multiple embedding models

3. **UI Improvements**
   - Web interface for agent monitoring
   - Rich CLI with progress bars
   - Better Telegram formatting

4. **Testing**
   - Unit tests for core modules
   - Integration tests for MCP servers
   - End-to-end testing framework

5. **Documentation**
   - More examples and tutorials
   - Video walkthroughs
   - API documentation

### Contribution Workflow

```bash
# 1. Fork the repository
# 2. Create feature branch
git checkout -b feature/amazing-feature

# 3. Make changes and commit
git commit -m "Add amazing feature"

# 4. Push to branch
git push origin feature/amazing-feature

# 5. Open Pull Request
```

---

## ğŸ“œ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **MCP Protocol**: [Model Context Protocol](https://github.com/modelcontextprotocol/python-sdk)
- **FastMCP**: Simplified MCP server implementation
- **python-telegram-bot**: Telegram Bot API wrapper
- **FAISS**: Facebook AI Similarity Search
- **LlamaIndex**: Document processing utilities
- **Trafilatura**: Web content extraction
- **PyMuPDF4LLM**: PDF processing for LLMs

---

## ğŸ“ Support

For questions, issues, or feature requests:

1. **Check Documentation**:
   - [QUICK_START.md](QUICK_START.md)
   - [TELEGRAM_SETUP_GUIDE.md](TELEGRAM_SETUP_GUIDE.md)
   - [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)

2. **Review Issues**: Check if your issue already exists

3. **Create New Issue**: Provide detailed description with:
   - Error messages
   - Steps to reproduce
   - Environment details (OS, Python version)

4. **Community**: Join discussions and share your use cases

---

## ğŸš€ Roadmap

### Version 1.1 (Planned)
- [ ] Multi-session support (concurrent users)
- [ ] Telegram commands (`/help`, `/status`, `/clear`)
- [ ] Voice message support
- [ ] Image analysis in Telegram

### Version 1.2 (Planned)
- [ ] Web UI dashboard
- [ ] User authentication and whitelisting
- [ ] Database integration (PostgreSQL/MongoDB)
- [ ] Enhanced error recovery

### Version 2.0 (Future)
- [ ] Multi-agent collaboration
- [ ] Workflow automation
- [ ] Plugin marketplace
- [ ] Cloud deployment templates

---

## ğŸ“Š Performance

**Typical Response Times:**
- Simple math query: 1-2 seconds
- Document search: 2-4 seconds
- Web search + content fetch: 5-10 seconds
- Complex multi-step query: 10-30 seconds

**Resource Usage:**
- RAM: 500MB - 2GB (depending on FAISS index size)
- CPU: Minimal (async I/O bound)
- Storage: 100MB base + document storage

---

## ğŸ” Security Considerations

1. **API Keys**: Never commit `.env` file to version control
2. **Telegram Bot**: Use bot token rotation if compromised
3. **Tool Execution**: `run_python_sandbox` has restricted imports
4. **Web Scraping**: Respect robots.txt and rate limits
5. **User Input**: Validate and sanitize all user inputs

---

## ğŸ“ˆ Stats

- **Lines of Code**: ~3,500
- **MCP Servers**: 4
- **Tools Available**: 35+
- **Supported File Types**: 10+
- **Transport Protocols**: STDIO, SSE

---

**Built with â¤ï¸ by the Cortex-R Team**

*Making AI agents accessible, powerful, and user-friendly.*

---

## ğŸ“ Learn More

- [MCP Documentation](https://modelcontextprotocol.io/)
- [FastMCP Guide](https://github.com/jlowin/fastmcp)
- [FAISS Tutorial](https://github.com/facebookresearch/faiss)
- [Telegram Bot API](https://core.telegram.org/bots/api)

---

**Last Updated**: November 9, 2025  
**Version**: 1.0.0  
**Status**: âœ… Production Ready

